{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3f1ca1282999>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfromnumeric\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfromnumeric\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefchararray\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mchar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mrecords\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOAA_TOKEN = 'USqkpElQPhaSddqJHBEXKBQCyDSGLdZV'\n",
    "\n",
    "EPA_EMAIL = 'ryla5068@colorado.edu'\n",
    "EPA_KEY = 'russetgoose87'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weather_data:\n",
    "    def __init__(self, token, startdate, enddate):\n",
    "        self.token = token\n",
    "        self.startdate = startdate\n",
    "        self.enddate = enddate\n",
    "        \n",
    "        print('Loading stations from pickle. This may take a minute...', end='')\n",
    "        self._load_stations()\n",
    "        if len(self.stations) == 0:\n",
    "            print('Not found.')\n",
    "        else:\n",
    "            num_stations_loaded = sum([len(i) for _, i in self.stations.items()])\n",
    "            print('Loaded {} stations.'.format(num_stations_loaded))\n",
    "        \n",
    "        self.datatypes = ['PRCP','WDFG','AWND','TAVG','TMAX','TMIN']\n",
    "        \n",
    "        for dtid in self.datatypes:\n",
    "            num_stored = len(self.stations.get(dtid, []))\n",
    "            _, num_available = self._get_stations(dtid, 0)\n",
    "            \n",
    "            print('For ' + dtid + ':\\t{} stored,\\t{} available'.format(num_stored, num_available))\n",
    "            \n",
    "            if num_stored == 0:\n",
    "                self.stations[dtid] = []\n",
    "            if num_stored < num_available:\n",
    "                print('\\tGetting stations. Starting with offset {}, going to {}\\n\\t'.format(num_stored, num_available), end='')\n",
    "            \n",
    "            \n",
    "            offset = num_stored\n",
    "            while offset < num_available:\n",
    "                response_json, _ = self._get_stations(dtid, offset)\n",
    "\n",
    "                for i in response_json['results']:\n",
    "                    self.stations[dtid].append({'id': i['id'], 'latlon': np.array([i['latitude'], i['longitude']]) })\n",
    "                \n",
    "                self._dump_stations()\n",
    "                \n",
    "                print('.', end='')\n",
    "                \n",
    "                offset += 1000\n",
    "            print('')\n",
    "        \n",
    "        num_stations_loaded = sum([len(i) for _, i in self.stations.items()])\n",
    "        print('{} total stations and locations stored!'.format(num_stations_loaded))\n",
    "        \n",
    "    def _load_stations(self):\n",
    "        try:\n",
    "            self.stations = pickle.load( open('stations.pckl', 'rb') )\n",
    "        except FileNotFoundError:\n",
    "            self.stations = {}\n",
    "            \n",
    "    def _dump_stations(self):\n",
    "        pickle.dump( self.stations, open('stations.pckl', 'wb') )\n",
    "        \n",
    "    def _get_stations(self, dtid, offset):\n",
    "        payload = {'datasetid': 'GHCND', 'datatypeid': dtid, 'startdate': self.startdate, 'enddate': self.enddate, 'limit': '1000', 'offset': offset}\n",
    "        head = {'token': self.token}\n",
    "        response = requests.get('https://www.ncdc.noaa.gov/cdo-web/api/v2/stations', headers=head, params=payload)\n",
    "        response_json = response.json()\n",
    "        \n",
    "        num_available = response_json['metadata']['resultset']['count']\n",
    "        \n",
    "        return response_json, num_available\n",
    "    def get_nearest_station(self, dtid, lat, lon):\n",
    "        target_loc = np.array([lat,lon])\n",
    "        dists = []\n",
    "        ids = []\n",
    "        for i in self.stations[dtid]:\n",
    "            dists.append( np.linalg.norm(i['latlon'] - target_loc) )\n",
    "            ids.append(i['id'])\n",
    "\n",
    "        val, idx = min((val, idx) for (idx, val) in enumerate(dists))\n",
    "        return ids[idx]\n",
    "\n",
    "    def get_nearest_weather(self, lat, lon, date_epa=None, date_noaa=None):\n",
    "        if not date_epa and not date_noaa:\n",
    "            raise AssertionError('Arguments date_epa and date_noaa cannot both be none.')\n",
    "        if not date_noaa:\n",
    "            date_noaa = date_epa[0:4] + '-' + date_epa[4:6] + '-' + date_epa[6:8]\n",
    "\n",
    "        data = {}\n",
    "        for dtid in self.datatypes:\n",
    "        #for dtid in ['PRCP','WDFG','AWND','TAVG','TMAX','TMIN']:\n",
    "\n",
    "            station_id = self.get_nearest_station(dtid, lat, lon)\n",
    "            #station_id = get_nearest_station(self, dtid, lat, lon)\n",
    "\n",
    "\n",
    "            payload = {'datasetid': 'GHCND', 'datatypeid': dtid, 'stationid': station_id, 'startdate': date_noaa, 'enddate': date_noaa}\n",
    "            head = {'token': self.token}\n",
    "            r = requests.get('https://www.ncdc.noaa.gov/cdo-web/api/v2/data', headers=head, params=payload)\n",
    "            rj = r.json()\n",
    "\n",
    "            try:\n",
    "                for i in rj['results']:\n",
    "                    datatype = i['datatype']\n",
    "                    data[datatype] = i['value']\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = weather_data(NOAA_TOKEN, '2017-01-10', '2019-12-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pollution_data:\n",
    "    def __init__(self, email, key, verbose=False):\n",
    "        self.email = email\n",
    "        self.key = key\n",
    "        \n",
    "        # Get codes and descriptions for pollution data\n",
    "        payload = {'email': self.email, 'key': self.key, 'pc': 'IMPROVE CARBON'}\n",
    "        r = requests.get('https://aqs.epa.gov/data/api/list/parametersByClass', params=payload)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Request made for IMPROVE CARBON codes and descriptions:')\n",
    "            print('URL:', r.url)\n",
    "            print('RESPONSE:', r, '\\n\\n')\n",
    "        \n",
    "        jr = r.json()\n",
    "        self.carbon_codes = []\n",
    "        self.carbon_descs = []\n",
    "        for i in jr['Data']:\n",
    "            self.carbon_codes.append(i['code'])\n",
    "            self.carbon_descs.append(i['value_represented'])\n",
    "        \n",
    "        if verbose:\n",
    "            print('Got IMPROVE CARBON codes and descriptions:')\n",
    "            for i in zip(self.carbon_codes, self.carbon_descs):\n",
    "                print(i)\n",
    "            print('\\n')\n",
    "            \n",
    "        # Set Location Bounds\n",
    "        self.loc_bounds = {'minlat': '27', 'maxlat': '50', 'minlon': '-130', 'maxlon': '-61'}\n",
    "        if verbose:\n",
    "            print('Created Location Bounds:', self.loc_bounds, '\\n\\n')\n",
    "        \n",
    "        # Create DataFrame\n",
    "        self.DF = pd.DataFrame(columns=['date'])\n",
    "        if verbose:\n",
    "            print('Created Dataframe.')\n",
    "            \n",
    "    def load_poll(self, verbose=False):\n",
    "        if verbose:\n",
    "            print('Loading pollution data from pickle...', end='')\n",
    "        try:\n",
    "            self.DF = pickle.load( open('poll.pckl', 'rb') )\n",
    "        except FileNotFoundError:\n",
    "            if verbose:\n",
    "                print('Pickle file not found. Use fetch_poll before using load_poll.')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('Done.')\n",
    "            \n",
    "            \n",
    "    def dump_poll(self):\n",
    "        pickle.dump( self.DF, open('poll.pckl', 'wb') )\n",
    "        \n",
    "    def fetch_poll(self, bdate, edate, verbose=False):\n",
    "        #for date in dates:\n",
    "        for param, desc in zip(self.carbon_codes, self.carbon_descs):\n",
    "            payload = {'email': self.email, 'key': self.key, 'param': param, 'bdate': bdate, 'edate': edate, 'minlat': self.loc_bounds['minlat'], 'maxlat': self.loc_bounds['maxlat'], 'minlon': self.loc_bounds['minlon'], 'maxlon': self.loc_bounds['maxlon']}\n",
    "            r = requests.get('https://aqs.epa.gov/data/api/dailyData/byBox', params=payload)\n",
    "\n",
    "            if verbose:\n",
    "                print('Request made for {} with code {}:'.format(desc, param))\n",
    "                #print('\\tURL:', r.url)\n",
    "                print('\\tRESPONSE:', r)\n",
    "\n",
    "            data = r.json()['Data']\n",
    "            columns = []\n",
    "            for i in data:\n",
    "                columns.append({'date': i['date_local'], 'site_number': i['site_number'], 'lat': i['latitude'], 'lon': i['longitude'], desc: i['arithmetic_mean']})\n",
    "            \n",
    "            if len(columns) > 0:\n",
    "                self.DF = self.DF.append(columns[0], ignore_index=True)\n",
    "                self.DF = self.DF.append(columns, ignore_index=True)\n",
    "            elif verbose:\n",
    "                print('\\t> No Rows Added')\n",
    "\n",
    "        self.DF = self.DF.groupby(['site_number', 'date']).max()\n",
    "        self.dump_poll()\n",
    "        print('Dumped Data')\n",
    "        \n",
    "    def load_weather(self, verbose=False):\n",
    "        if verbose:\n",
    "            print('Loading weather data from pickle...', end='')\n",
    "        try:\n",
    "            self.DF = pickle.load( open('weather.pckl', 'rb') )\n",
    "        except FileNotFoundError:\n",
    "            if verbose:\n",
    "                print('Pickle file not found. Use fetch_weather before using load_weather.')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('Done.')\n",
    "            \n",
    "    def dump_weather(self):\n",
    "        pickle.dump( self.DF, open('weather.pckl', 'wb') )\n",
    "        \n",
    "    def fetch_weather(self, wd, verbose=False):\n",
    "        if verbose:\n",
    "            print('Fetching weather data. ', end='')\n",
    "        \n",
    "        i=1\n",
    "        for index, row in self.DF.iterrows():\n",
    "            site_number = index[0]\n",
    "            date = index[1]\n",
    "            lat = row.lat\n",
    "            lon = row.lon\n",
    "\n",
    "            weather = wd.get_nearest_weather(lat, lon, date_noaa=date)\n",
    "\n",
    "            for datatype, value in weather.items():\n",
    "                self.DF.loc[index, datatype] = value\n",
    "            \n",
    "            if i%20 == 0:\n",
    "                self.dump_weather()\n",
    "                if verbose:\n",
    "                    print('dumped', end='')\n",
    "            if verbose:\n",
    "                print('.', end='')\n",
    "            i += 1\n",
    "            \n",
    "\n",
    "        print('\\nDone.')\n",
    "        \n",
    "    def clean_data(self):\n",
    "        self.DF = self.DF.fillna(self.DF.mean())\n",
    "        \n",
    "    def get_train_test(self):\n",
    "        \n",
    "        # Randomly split 20% of data for validation, but keep datapoints from same station in same dataset.\n",
    "        sites = np.unique([index[0] for index, _ in self.DF.iterrows()])\n",
    "        sites = np.random.permutation(sites)\n",
    "        split_sites = [[],[],[],[],[]]\n",
    "        for i in range(len(sites)):\n",
    "            s = i%len(split_sites)\n",
    "            split_sites[s].append(sites[i])\n",
    "        splits = []\n",
    "        for i in split_sites:\n",
    "            splits.append(poll_data.DF.loc[i])\n",
    "        take = 1\n",
    "        train_df = None\n",
    "        test_df = None\n",
    "        for i in range(len(splits)):\n",
    "            if i == take:\n",
    "                test_df = splits[i]\n",
    "            else:\n",
    "                train_df = pd.concat([train_df, splits[i]])\n",
    "                \n",
    "                \n",
    "        # Move date data from index to column\n",
    "        train_df.reset_index(drop=False, inplace=True)\n",
    "        test_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "        \n",
    "        # Convert date data to float\n",
    "        epoch = pd.Timestamp('2014-01-01')\n",
    "\n",
    "        train_dt = pd.to_datetime(train_df.date, yearfirst=True)\n",
    "        train_df.loc[:,'date'] = (train_dt-epoch).astype('timedelta64[h]')\n",
    "\n",
    "        test_dt = pd.to_datetime(test_df.date, yearfirst=True)\n",
    "        test_df.loc[:,'date'] = (test_dt-epoch).astype('timedelta64[h]')\n",
    "\n",
    "        \n",
    "        # Split into x and y\n",
    "        y_cols =  ['OC PM2.5 LC TOR', 'EC PM2.5 LC TOR', 'OC1 PM2.5 LC',\n",
    "       'OC2 PM2.5 LC', 'OC3 PM2.5 LC', 'OC4 PM2.5 LC', 'OP PM2.5 LC TOR',\n",
    "       'EC1 PM2.5 LC', 'EC2 PM2.5 LC', 'EC3 PM2.5 LC']\n",
    "\n",
    "        train_x = train_df.drop(y_cols, 'columns')\n",
    "        train_x = train_x.drop('site_number', 'columns')\n",
    "        train_y = train_df[y_cols]\n",
    "\n",
    "        test_x = test_df.drop(y_cols, 'columns')\n",
    "        test_x = test_x.drop('site_number', 'columns')\n",
    "        test_y = test_df[y_cols]\n",
    "\n",
    "        # Norm train data\n",
    "        x_mean = train_x.mean()\n",
    "        x_std = train_x.std()\n",
    "        train_x = (train_x - x_mean) / x_std\n",
    "        test_x = (test_x - x_mean) / x_std\n",
    "        \n",
    "        self.X_train = train_x.values\n",
    "        self.y_train = train_y.values\n",
    "        self.X_test = test_x.values\n",
    "        self.y_test = test_y.values\n",
    "        \n",
    "        return self.X_train, self.y_train, self.X_test, self.y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_data = pollution_data(EPA_EMAIL, EPA_KEY, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#poll_data.fetch_poll(bdate='20190118', edate='20190129', verbose=True)\n",
    "#poll_data.fetch_poll(bdate='20190118', edate='20190118', verbose=True)\n",
    "poll_data.load_poll(verbose=True)\n",
    "#poll_data.fetch_weather(wd, verbose=True)\n",
    "poll_data.load_weather(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_data.DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_data.clean_data()\n",
    "X_train, y_train, X_test, y_test = poll_data.get_train_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=[9]),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "\n",
    "history = model.fit(\n",
    "  X_train, y_train, epochs=EPOCHS, validation_data = (X_test, y_test), verbose=0,\n",
    "  callbacks=[tfdocs.modeling.EpochDots()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "\n",
    "plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)\n",
    "plotter.plot({'Basic': history}, metric = \"mae\")\n",
    "#plt.ylim([0, 10])\n",
    "#plt.ylabel('MAE [MPG]')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cols =  ['OC PM2.5 LC TOR', 'EC PM2.5 LC TOR', 'OC1 PM2.5 LC',\n",
    "       'OC2 PM2.5 LC', 'OC3 PM2.5 LC', 'OC4 PM2.5 LC', 'OP PM2.5 LC TOR',\n",
    "       'EC1 PM2.5 LC', 'EC2 PM2.5 LC', 'EC3 PM2.5 LC']\n",
    "poll_data.DF.loc[:,y_cols].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_data.DF.loc[:,y_cols].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_data.clean_data()\n",
    "X_train, y_train, X_test, y_test = poll_data.get_train_test()\n",
    "##SKLearn Multilinear Regression\n",
    "y_train = y_train[:, 0]\n",
    "y_test = y_test[:, 0]\n",
    "\n",
    "RMSEDegree = []\n",
    "for i in range(5):\n",
    "    linreg = Pipeline([('poly', PolynomialFeatures(degree = i)), ('linear', LinearRegression(fit_intercept=False))])\n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_pred = linreg.predict(X_test)\n",
    "    whys = []\n",
    "    for i in range(len(y_pred)):\n",
    "        whys.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    RMSEDegree.append(np.average(whys))\n",
    "plt.ylim([0, 5])\n",
    "plt.plot([0, 1, 2, 3, 4], RMSEDegree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
